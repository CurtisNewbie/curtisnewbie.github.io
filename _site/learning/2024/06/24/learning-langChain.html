<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Learning LangChain | somewhere in the ocean</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Learning LangChain" />
<meta name="author" content="Yongjie Zhuang" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Relevant Sites" />
<meta property="og:description" content="Relevant Sites" />
<link rel="canonical" href="https://curtisnewbie.github.io/learning/2024/06/24/learning-langChain.html" />
<meta property="og:url" content="https://curtisnewbie.github.io/learning/2024/06/24/learning-langChain.html" />
<meta property="og:site_name" content="somewhere in the ocean" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-06-24T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Learning LangChain" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Yongjie Zhuang"},"dateModified":"2024-06-24T00:00:00+08:00","datePublished":"2024-06-24T00:00:00+08:00","description":"Relevant Sites","headline":"Learning LangChain","mainEntityOfPage":{"@type":"WebPage","@id":"https://curtisnewbie.github.io/learning/2024/06/24/learning-langChain.html"},"url":"https://curtisnewbie.github.io/learning/2024/06/24/learning-langChain.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://curtisnewbie.github.io/feed.xml" title="somewhere in the ocean" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">somewhere in the ocean</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">about</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Learning LangChain</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2024-06-24T00:00:00+08:00" itemprop="datePublished">Jun 24, 2024
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="relevant-sites">Relevant Sites</h2>

<ul>
  <li><a href="https://python.langchain.com/v0.2/docs/introduction/">LangChain v0.2 Doc</a></li>
  <li><a href="https://github.com/langchain-ai/langchain">LangChain Github</a></li>
  <li><a href="https://huggingface.co/docs/hub/en/models-downloading">Download HuggingFace Model</a></li>
  <li><a href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct">HuggingFace - Microsoft/Phi-3-mini-4k-instruct</a></li>
  <li><a href="https://huggingface.co/blog/langchain">Blog - HuggingFace LangChain Partner Package</a></li>
  <li><a href="https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0">HuggingFace - TinyLlama/TinyLlama-1.1B-Chat-v1.0</a></li>
  <li><a href="https://discuss.huggingface.co/t/llama-7b-gpu-memory-requirement/34323/7">HuggingFace Discussion - LLaMA 7B GPU Memory Requirement</a></li>
</ul>

<h2 id="getting-started">Getting Started</h2>

<p>Install langchain</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> pip <span class="nb">install </span>langchain
</code></pre></div></div>

<p>Setup langsmith (https://smith.langchain.com/), but it’s not really needed, we may well just skip this.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">LANGCHAIN_TRACING_V2</span><span class="o">=</span><span class="s2">"true"</span>
<span class="nb">export </span><span class="nv">LANGCHAIN_API_KEY</span><span class="o">=</span><span class="s2">"..."</span>
</code></pre></div></div>

<p>Use LangChain with HuggingFace</p>

<ul>
  <li>https://python.langchain.com/v0.2/docs/integrations/platforms/huggingface/</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> pip <span class="nb">install </span>langchain-huggingface
python3 <span class="nt">-m</span> pip <span class="nb">install</span> <span class="nt">--upgrade</span> <span class="nt">--quiet</span>  transformers <span class="nt">--quiet</span>
</code></pre></div></div>

<p>Load the HuggingFace model locally (TinyLlama/TinyLlama-1.1B-Chat-v1.0), it’s a 1.1b model, I am running it on Macbook Pro M2 16GB. My laptop cannot handle Model with over 3b parameters.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFacePipeline</span>

<span class="n">hf</span> <span class="o">=</span> <span class="n">HuggingFacePipeline</span><span class="p">.</span><span class="n">from_model_id</span><span class="p">(</span>
    <span class="n">model_id</span><span class="o">=</span><span class="s">"TinyLlama/TinyLlama-1.1B-Chat-v1.0"</span><span class="p">,</span>
    <span class="n">task</span><span class="o">=</span><span class="s">"text-generation"</span><span class="p">,</span>
    <span class="n">pipeline_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s">"max_new_tokens"</span><span class="p">:</span> <span class="mi">150</span><span class="p">},</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Task is what the model can handle. In this case, we are using a <code class="language-plaintext highlighter-rouge">text-generation</code> model. Which task should we specify depends on what we are doing, a model may be capable of multiple tasks.</p>

<p><img src="/assets/images/hugging-face-search-task.png" alt="assets/images/hugging-face-search-task.png" /></p>

<blockquote>
  <p>Searching models based on tasks on HuggingFace.</p>
</blockquote>

<p>Create Prompt Template (will get much better response):</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="c1"># prompt template
</span><span class="n">template</span> <span class="o">=</span> <span class="s">"""Question: {question}

Answer: Let's think step by step."""</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>

<span class="c1"># bind local langchain to the prompt
</span><span class="n">chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">hf</span><span class="p">.</span><span class="n">bind</span><span class="p">()</span>

<span class="n">q</span> <span class="o">=</span> <span class="s">"How to brew coffee?"</span>
<span class="k">print</span><span class="p">(</span><span class="n">chain</span><span class="p">.</span><span class="n">invoke</span><span class="p">({</span><span class="s">"question"</span><span class="p">:</span> <span class="n">q</span><span class="p">}))</span>
</code></pre></div></div>

<p>We can also invoke the model directly without PromptTemplate, but the response is worse:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">hf</span><span class="p">.</span><span class="n">invoke</span><span class="p">(</span><span class="s">"How to brew coffee?"</span><span class="p">))</span>
</code></pre></div></div>

<p>A working example:</p>

<p>gist: https://gist.github.com/CurtisNewbie/2b25f811b5b177548207488b2b409dbf</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">readline</span>
<span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFacePipeline</span>

<span class="n">hf</span> <span class="o">=</span> <span class="n">HuggingFacePipeline</span><span class="p">.</span><span class="n">from_model_id</span><span class="p">(</span>
    <span class="n">model_id</span><span class="o">=</span><span class="s">"TinyLlama/TinyLlama-1.1B-Chat-v1.0"</span><span class="p">,</span>
    <span class="n">task</span><span class="o">=</span><span class="s">"text-generation"</span><span class="p">,</span>
    <span class="n">pipeline_kwargs</span><span class="o">=</span><span class="p">{</span>
        <span class="s">"max_new_tokens"</span><span class="p">:</span> <span class="mi">150</span><span class="p">,</span>
        <span class="s">"temperature"</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
        <span class="s">"top_k"</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
        <span class="s">"top_p"</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">,</span>
        <span class="s">"do_sample"</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span>

<span class="n">template</span> <span class="o">=</span> <span class="s">"""Question: {question}

Answer: Let's think step by step."""</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>

<span class="n">chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">hf</span><span class="p">.</span><span class="n">bind</span><span class="p">()</span>

<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Enter your question:"</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span><span class="p">().</span><span class="n">strip</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">q</span><span class="p">:</span> <span class="k">continue</span>
        <span class="n">resp</span> <span class="o">=</span> <span class="n">chain</span><span class="p">.</span><span class="n">invoke</span><span class="p">({</span><span class="s">"question"</span><span class="p">:</span> <span class="n">q</span><span class="p">})</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n\n</span><span class="s">&gt; model: '</span><span class="si">{</span><span class="n">resp</span><span class="si">}</span><span class="s">'</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>

    <span class="k">except</span> <span class="nb">InterruptedError</span><span class="p">:</span>
        <span class="n">sys</span><span class="p">.</span><span class="nb">exit</span><span class="p">()</span>
    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Exception caught </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="terminology">Terminology</h2>

<ul>
  <li><code class="language-plaintext highlighter-rouge">HF Format</code>: Hugging Face Format</li>
</ul>

  </div><a class="u-url" href="/learning/2024/06/24/learning-langChain.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">somewhere in the ocean</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Yongjie Zhuang</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/CurtisNewbie"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">CurtisNewbie</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Til the end</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
