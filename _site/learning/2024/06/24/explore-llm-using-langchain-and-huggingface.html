<!-- https://github.com/jekyll/minima/tree/master/_layouts -->
<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Explore LLM using LangChain and HuggingFace | kekw</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Explore LLM using LangChain and HuggingFace" />
<meta name="author" content="Yongjie Zhuang" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Relevant Sites" />
<meta property="og:description" content="Relevant Sites" />
<link rel="canonical" href="https://curtisnewbie.github.io/learning/2024/06/24/explore-llm-using-langchain-and-huggingface.html" />
<meta property="og:url" content="https://curtisnewbie.github.io/learning/2024/06/24/explore-llm-using-langchain-and-huggingface.html" />
<meta property="og:site_name" content="kekw" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-06-24T09:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Explore LLM using LangChain and HuggingFace" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Yongjie Zhuang"},"dateModified":"2024-06-24T09:00:00+08:00","datePublished":"2024-06-24T09:00:00+08:00","description":"Relevant Sites","headline":"Explore LLM using LangChain and HuggingFace","mainEntityOfPage":{"@type":"WebPage","@id":"https://curtisnewbie.github.io/learning/2024/06/24/explore-llm-using-langchain-and-huggingface.html"},"url":"https://curtisnewbie.github.io/learning/2024/06/24/explore-llm-using-langchain-and-huggingface.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://curtisnewbie.github.io/feed.xml" title="kekw" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">kekw</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <!-- https://github.com/jekyll/minima/tree/master/_layouts -->
<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Explore LLM using LangChain and HuggingFace</h1>
    <p class="post-meta"><time class="dt-published" datetime="2024-06-24T09:00:00+08:00" itemprop="datePublished">
        Jun 24, 2024 &nbsp&nbsp&nbsp&nbsp4132 Words
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="relevant-sites">Relevant Sites</h2>

<ul>
  <li><a href="https://python.langchain.com/v0.2/docs/introduction/">LangChain v0.2 Doc</a></li>
  <li><a href="https://github.com/langchain-ai/langchain">LangChain Github</a></li>
  <li><a href="https://huggingface.co/docs/hub/en/models-downloading">Download HuggingFace Model</a></li>
  <li><a href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct">HuggingFace - Microsoft/Phi-3-mini-4k-instruct</a></li>
  <li><a href="https://huggingface.co/blog/langchain">Blog - HuggingFace LangChain Partner Package</a></li>
  <li><a href="https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0">HuggingFace - TinyLlama/TinyLlama-1.1B-Chat-v1.0</a></li>
  <li><a href="https://huggingface.co/TinyLlama/TinyLlama_v1.1">HuggingFace - TinyLlama/TinyLlama_v1.1</a></li>
  <li><a href="https://huggingface.co/TinyLlama/TinyLlama_v1.1_chinese">HuggingFace - TinyLlama/TinyLlama_v1.1_chinese</a></li>
  <li><a href="https://discuss.huggingface.co/t/llama-7b-gpu-memory-requirement/34323/7">HuggingFace Discussion - LLaMA 7B GPU Memory Requirement</a></li>
  <li><a href="https://huggingface.co/docs/hub/en/models-downloading">HuggingFace - Download Modesl</a></li>
  <li><a href="https://python.langchain.com/v0.2/docs/tutorials/chatbot/">LangChain - Chatbot</a></li>
  <li><a href="https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/">LangChain - Conversational RAG</a></li>
  <li><a href="https://python.langchain.com/v0.2/docs/tutorials/rag/">LangChain - Build a Retrieval Augmented Generation (RAG) App</a></li>
  <li><a href="https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/">LangChain - Chroma (Vector Store)</a></li>
  <li><a href="https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/">LangChain - Vector Stores</a></li>
  <li><a href="https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int4">HuggingFace - Qwen/Qwen-1_8B-Chat-Int4</a></li>
  <li><a href="https://github.com/QwenLM/Qwen?tab=readme-ov-file#quantization">Github - QwenLM/Qwen</a></li>
  <li><a href="https://gist.github.com/CurtisNewbie/9d220701b4dd7f3ce00e728317ca1436">Gist CurtisNewbie - Qwen/Qwen-1_8B-Chat-Int4 Demo</a></li>
  <li><a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes">HuggingFace - 4bit Quantization</a></li>
  <li><a href="https://huggingface.co/docs/bitsandbytes/main/en/installation">HuggingFace - bitsandbytes for Quantization</a></li>
  <li><a href="https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/vectorstore/">LangChain - Vector store-backed retriever</a></li>
  <li><a href="https://github.com/zylon-ai/private-gpt">Private GPT</a></li>
</ul>

<h2 id="getting-started">Getting Started</h2>

<p>Install langchain</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> pip <span class="nb">install </span>langchain
</code></pre></div></div>

<p>Setup langsmith (https://smith.langchain.com/), but it’s not really needed, we may well just skip this.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">LANGCHAIN_TRACING_V2</span><span class="o">=</span><span class="s2">"true"</span>
<span class="nb">export </span><span class="nv">LANGCHAIN_API_KEY</span><span class="o">=</span><span class="s2">"..."</span>
</code></pre></div></div>

<p>Use LangChain with HuggingFace</p>

<ul>
  <li>https://python.langchain.com/v0.2/docs/integrations/platforms/huggingface/</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> pip <span class="nb">install </span>langchain-huggingface
python3 <span class="nt">-m</span> pip <span class="nb">install</span> <span class="nt">--upgrade</span> <span class="nt">--quiet</span>  transformers <span class="nt">--quiet</span>
</code></pre></div></div>

<p>Load the HuggingFace model locally (TinyLlama/TinyLlama-1.1B-Chat-v1.0), it’s a 1.1b model, I am running it on Macbook Pro M2 16GB. My laptop cannot handle Model with over 3b parameters.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFacePipeline</span>

<span class="n">hf</span> <span class="o">=</span> <span class="n">HuggingFacePipeline</span><span class="p">.</span><span class="n">from_model_id</span><span class="p">(</span>
    <span class="n">model_id</span><span class="o">=</span><span class="s">"TinyLlama/TinyLlama-1.1B-Chat-v1.0"</span><span class="p">,</span>
    <span class="n">task</span><span class="o">=</span><span class="s">"text-generation"</span><span class="p">,</span>
    <span class="n">pipeline_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s">"max_new_tokens"</span><span class="p">:</span> <span class="mi">150</span><span class="p">},</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Task is what the model can handle. In this case, we are using a <code class="language-plaintext highlighter-rouge">text-generation</code> model. Which task should we specify depends on what we are doing, a model may be capable of multiple tasks.</p>

<p><img src="/assets/images/hugging-face-search-task.png" alt="assets/images/hugging-face-search-task.png" /></p>

<blockquote>
  <p>Searching models based on tasks on HuggingFace.</p>
</blockquote>

<p>Create Prompt Template (will get much better response):</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="c1"># prompt template
</span><span class="n">template</span> <span class="o">=</span> <span class="s">"""Question: {question}

Answer: Let's think step by step."""</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>

<span class="c1"># bind local langchain to the prompt
</span><span class="n">chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">hf</span><span class="p">.</span><span class="n">bind</span><span class="p">()</span>

<span class="n">q</span> <span class="o">=</span> <span class="s">"How to brew coffee?"</span>
<span class="k">print</span><span class="p">(</span><span class="n">chain</span><span class="p">.</span><span class="n">invoke</span><span class="p">({</span><span class="s">"question"</span><span class="p">:</span> <span class="n">q</span><span class="p">}))</span>
</code></pre></div></div>

<p>We can also invoke the model directly without PromptTemplate, but the response is worse:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">hf</span><span class="p">.</span><span class="n">invoke</span><span class="p">(</span><span class="s">"How to brew coffee?"</span><span class="p">))</span>
</code></pre></div></div>

<p>A working example is available at: <a href="https://github.com/CurtisNewbie/llm_stuff/blob/main/tinyllama.py">github.com/CurtisNewbie/llm_stuff/blob/main/tinyllama.py</a></p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">readline</span>
<span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFacePipeline</span>

<span class="n">hf</span> <span class="o">=</span> <span class="n">HuggingFacePipeline</span><span class="p">.</span><span class="n">from_model_id</span><span class="p">(</span>
    <span class="n">model_id</span><span class="o">=</span><span class="s">"TinyLlama/TinyLlama-1.1B-Chat-v1.0"</span><span class="p">,</span>
    <span class="n">task</span><span class="o">=</span><span class="s">"text-generation"</span><span class="p">,</span>
    <span class="n">pipeline_kwargs</span><span class="o">=</span><span class="p">{</span>
        <span class="s">"max_new_tokens"</span><span class="p">:</span> <span class="mi">150</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="n">model_kwargs</span><span class="o">=</span><span class="p">{</span>
        <span class="s">"temperature"</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
        <span class="s">"top_k"</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
        <span class="s">"top_p"</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">,</span>
        <span class="s">"do_sample"</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span>

<span class="n">template</span> <span class="o">=</span> <span class="s">"""Question: {question}

Answer: Let's think step by step."""</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>

<span class="n">chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">hf</span><span class="p">.</span><span class="n">bind</span><span class="p">()</span>

<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Enter your question:"</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">sys</span><span class="p">.</span><span class="n">stdin</span><span class="p">.</span><span class="n">readline</span><span class="p">().</span><span class="n">strip</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">q</span><span class="p">:</span> <span class="k">continue</span>
        <span class="n">resp</span> <span class="o">=</span> <span class="n">chain</span><span class="p">.</span><span class="n">invoke</span><span class="p">({</span><span class="s">"question"</span><span class="p">:</span> <span class="n">q</span><span class="p">})</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n\n</span><span class="s">&gt; model: '</span><span class="si">{</span><span class="n">resp</span><span class="si">}</span><span class="s">'</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>

    <span class="k">except</span> <span class="nb">InterruptedError</span><span class="p">:</span>
        <span class="n">sys</span><span class="p">.</span><span class="nb">exit</span><span class="p">()</span>
    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Exception caught </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<p>If the model supports streaming, we can change the code like the following. If the model doesn’t support streaming then the method call is simply blocked returning only one single chunk:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">chain</span><span class="p">.</span><span class="n">stream</span><span class="p">({</span><span class="s">"question"</span><span class="p">:</span> <span class="n">q</span><span class="p">}):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">chunk</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="retrieval-augmented-generation-rag">Retrieval Augmented Generation (RAG)</h2>

<ul>
  <li><a href="https://python.langchain.com/v0.2/docs/tutorials/rag/">LangChain - Build a Retrieval Augmented Generation (RAG) App</a></li>
  <li><a href="https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/">LangChain - Chroma (Vector Store)</a></li>
</ul>

<p>RAG is a way to connect LLM model with external sources. In LangChain’s RAG tutorial, an OpenAI model is used and connected to a online document parsed using bs4.</p>

<p>In essence, RAG involves indexing the data (documents), storing the indexes in vector database, retrieving the context from the vector database (based on similarity,
i.e., Similarity Search) for the question, and finally adding the context to the prompt that is passed to the LLM model.</p>

<p>The following images are from LangChain.</p>

<p><img src="https://python.langchain.com/v0.2/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png" height="350px" /></p>

<p><img src="https://python.langchain.com/v0.2/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png" height="350px" /></p>

<p>Install relevant dependencies. Chroma is a vector database.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">python3</span> <span class="o">-</span><span class="n">m</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">langchain</span> <span class="n">langchain_community</span> <span class="n">langchain_chroma</span>
</code></pre></div></div>

<p>First of all, we create a LangChain pipeline for the model:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">traceback</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">readline</span>
<span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFacePipeline</span>

<span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">300</span>
<span class="n">task</span><span class="o">=</span><span class="s">"text-generation"</span>
<span class="n">model</span><span class="o">=</span><span class="s">"TinyLlama/TinyLlama-1.1B-Chat-v1.0"</span>

<span class="n">hf</span> <span class="o">=</span> <span class="n">HuggingFacePipeline</span><span class="p">.</span><span class="n">from_model_id</span><span class="p">(</span>
    <span class="n">model_id</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span>
    <span class="n">pipeline_kwargs</span><span class="o">=</span><span class="p">{</span>
        <span class="s">"max_new_tokens"</span><span class="p">:</span> <span class="n">max_new_tokens</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="n">model_kwargs</span><span class="o">=</span><span class="p">{</span>
        <span class="s">"temperature"</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
        <span class="s">"top_k"</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
        <span class="s">"top_p"</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">,</span>
        <span class="s">"do_sample"</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Import DocumentLoader to load external documents. Import Splitter to break documents into chunks. Import Embeddings to create a vector representation of text that is stored in a vector database.</p>

<p>In the following code, the retriever is created from the vector database. Retriever is simply a concept that accepts a string query and returns a list of documents,
in this case, it’s doing similarity search based on the question asked.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">langchain_chroma</span> <span class="kn">import</span> <span class="n">Chroma</span>
<span class="kn">from</span> <span class="nn">langchain_community.document_loaders</span> <span class="kn">import</span> <span class="n">TextLoader</span>
<span class="kn">from</span> <span class="nn">langchain_text_splitters</span> <span class="kn">import</span> <span class="n">CharacterTextSplitter</span>
<span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>
<span class="kn">from</span> <span class="nn">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnablePassthrough</span>

<span class="c1"># load the local document
</span><span class="n">files</span> <span class="o">=</span> <span class="p">[</span><span class="s">"about_onecafe.txt"</span><span class="p">]</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span> <span class="n">documents</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">TextLoader</span><span class="p">(</span><span class="n">f</span><span class="p">).</span><span class="n">load</span><span class="p">())</span>

<span class="c1"># split documents into chunks
</span><span class="n">text_splitter</span> <span class="o">=</span> <span class="n">CharacterTextSplitter</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="p">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>

<span class="c1"># create Embedding function to convert each piece of text to vector
</span><span class="n">embed</span> <span class="o">=</span> <span class="n">HuggingFaceEmbeddings</span><span class="p">()</span>

<span class="c1"># store documents into Chroma (in memory)
</span><span class="n">vec</span> <span class="o">=</span> <span class="n">Chroma</span><span class="p">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">embed</span><span class="p">)</span>

<span class="c1"># create retriever from the vector database
</span><span class="n">retriever</span> <span class="o">=</span> <span class="n">vec</span><span class="p">.</span><span class="n">as_retriever</span><span class="p">(</span><span class="n">search_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s">"k"</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span> <span class="c1"># default: k is 4
</span></code></pre></div></div>

<p>With all the Loader, Splitter, Embeddings, Vector Database and Retriever setup, we can construct a chain that automatically complete the context for the LLM model.</p>

<p>The prompt template is slightly different, it now contains a context section for the LLM model (It’s copied from LangSmith). The content of <code class="language-plaintext highlighter-rouge">{context}</code> actually comes from the vector database using the Retriever that we just created.</p>

<p>The <code class="language-plaintext highlighter-rouge">RunnablePassthrough()</code> does nothing, it just passes the query to <code class="language-plaintext highlighter-rouge">{question}</code> section.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">format_docs</span><span class="p">(</span><span class="n">docs</span><span class="p">):</span>
    <span class="k">return</span> <span class="s">"</span><span class="se">\n\n</span><span class="s">"</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">doc</span><span class="p">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">)</span>

<span class="n">template</span> <span class="o">=</span> <span class="s">"""You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.

Question: {question}

Context: {context}

Answer:"""</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>

<span class="n">chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span><span class="s">"context"</span><span class="p">:</span> <span class="n">retriever</span> <span class="o">|</span> <span class="n">format_docs</span><span class="p">,</span> <span class="s">"question"</span><span class="p">:</span> <span class="n">RunnablePassthrough</span><span class="p">()}</span>
    <span class="o">|</span> <span class="n">prompt</span>
    <span class="o">|</span> <span class="n">hf</span><span class="p">.</span><span class="n">bind</span><span class="p">()</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Finally, we just invoke the chain with our question:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">chain</span><span class="p">.</span><span class="n">invoke</span><span class="p">(</span><span class="s">"What is LLM model?"</span><span class="p">))</span>
</code></pre></div></div>

<p>A working example is available at: <a href="https://github.com/CurtisNewbie/llm_stuff/blob/main/tinyllama_rag.py">github.com/CurtisNewbie/llm_stuff/blob/main/tinyllama_rag.py</a></p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">traceback</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">readline</span>
<span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFacePipeline</span>

<span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">300</span>
<span class="n">task</span><span class="o">=</span><span class="s">"text-generation"</span>
<span class="n">model</span><span class="o">=</span><span class="s">"TinyLlama/TinyLlama-1.1B-Chat-v1.0"</span>

<span class="n">hf</span> <span class="o">=</span> <span class="n">HuggingFacePipeline</span><span class="p">.</span><span class="n">from_model_id</span><span class="p">(</span>
    <span class="n">model_id</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span>
    <span class="n">pipeline_kwargs</span><span class="o">=</span><span class="p">{</span>
        <span class="s">"max_new_tokens"</span><span class="p">:</span> <span class="n">max_new_tokens</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="n">model_kwargs</span><span class="o">=</span><span class="p">{</span>
        <span class="s">"temperature"</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
        <span class="s">"top_k"</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
        <span class="s">"top_p"</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">,</span>
        <span class="s">"do_sample"</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="nn">langchain_chroma</span> <span class="kn">import</span> <span class="n">Chroma</span>
<span class="kn">from</span> <span class="nn">langchain_community.document_loaders</span> <span class="kn">import</span> <span class="n">TextLoader</span>
<span class="kn">from</span> <span class="nn">langchain_text_splitters</span> <span class="kn">import</span> <span class="n">CharacterTextSplitter</span>
<span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>
<span class="kn">from</span> <span class="nn">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnablePassthrough</span>

<span class="c1"># load the document and split it into chunks
</span><span class="n">files</span> <span class="o">=</span> <span class="p">[</span><span class="s">"about_onecafe.txt"</span><span class="p">]</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>
    <span class="n">documents</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">TextLoader</span><span class="p">(</span><span class="n">f</span><span class="p">).</span><span class="n">load</span><span class="p">())</span>

<span class="c1"># split it into chunks
</span><span class="n">text_splitter</span> <span class="o">=</span> <span class="n">CharacterTextSplitter</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="p">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>

<span class="c1"># create the open-source embedding function (also a model)
</span><span class="n">embed</span> <span class="o">=</span> <span class="n">HuggingFaceEmbeddings</span><span class="p">()</span>

<span class="c1"># load it into Chroma
</span><span class="n">vec</span> <span class="o">=</span> <span class="n">Chroma</span><span class="p">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">embed</span><span class="p">)</span>
<span class="n">reti</span> <span class="o">=</span> <span class="n">vec</span><span class="p">.</span><span class="n">as_retriever</span><span class="p">(</span><span class="n">search_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s">"k"</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span> <span class="c1"># default: k is 4
</span>
<span class="k">def</span> <span class="nf">format_docs</span><span class="p">(</span><span class="n">docs</span><span class="p">):</span>
    <span class="k">return</span> <span class="s">"</span><span class="se">\n\n</span><span class="s">"</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">doc</span><span class="p">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">)</span>

<span class="n">template</span> <span class="o">=</span> <span class="s">"""You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.

Question: {question}

Context: {context}

Answer:"""</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>

<span class="n">chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span><span class="s">"context"</span><span class="p">:</span> <span class="n">reti</span> <span class="o">|</span> <span class="n">format_docs</span><span class="p">,</span> <span class="s">"question"</span><span class="p">:</span> <span class="n">RunnablePassthrough</span><span class="p">()}</span>
    <span class="o">|</span> <span class="n">prompt</span>
    <span class="o">|</span> <span class="n">hf</span><span class="p">.</span><span class="n">bind</span><span class="p">()</span>
<span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">chain</span><span class="p">.</span><span class="n">invoke</span><span class="p">(</span><span class="s">"Tell me about onecafe"</span><span class="p">))</span>
</code></pre></div></div>

<h2 id="quantization">Quantization</h2>

<ul>
  <li><a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes">HuggingFace - 4bit Quantization</a></li>
  <li><a href="https://huggingface.co/docs/bitsandbytes/main/en/installation">HuggingFace - bitsandbytes for Quantization</a></li>
</ul>

<p>With Quantization (e.g., 4bit), we can reduce the usage of memory, making LLM runs faster. Some models on HuggingFace have already done it without extra configuration, but those may require CUDA support.</p>

<p>Quantization only works on GPU (kinda), include <code class="language-plaintext highlighter-rouge">load_in_4bit=True</code> and <code class="language-plaintext highlighter-rouge">device_map=auto</code> to enable the quantization, but model should support it in the first place.</p>

<p>Quantization needs bitsandbytes:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">python3</span> <span class="o">-</span><span class="n">m</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">bitsandbytes</span>
</code></pre></div></div>

<p>When we create the model, include the args mentioned above:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">hf</span> <span class="o">=</span> <span class="n">HuggingFacePipeline</span><span class="p">.</span><span class="n">from_model_id</span><span class="p">(</span>
    <span class="n">model_id</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span>
    <span class="n">pipeline_kwargs</span><span class="o">=</span><span class="p">{</span>
        <span class="s">"max_new_tokens"</span><span class="p">:</span> <span class="n">max_new_tokens</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="n">model_kwargs</span><span class="o">=</span><span class="p">{</span>
        <span class="s">"temperature"</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
        <span class="s">"top_k"</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
        <span class="s">"top_p"</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">,</span>
        <span class="s">"do_sample"</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
        <span class="s">"load_in_4bit"</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span> <span class="c1"># 4bit quantization
</span>        <span class="s">"device_map"</span><span class="p">:</span><span class="s">"auto"</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span>
</code></pre></div></div>

<h2 id="demo---deploy-llm-model-on-centos-with-gpu">Demo - Deploy LLM Model on CentOS with GPU</h2>

<p>This section roughly documents how I setup CUDA and all sort of dependencies to run the LLM Model: <code class="language-plaintext highlighter-rouge">Qwen/Qwen2-7B-Instruct</code> on CentOS
(the setup is roughly the same even if you change to another LLM model).</p>

<p><strong><em>The python code is available in repo: <a href="https://github.com/CurtisNewbie/llm_stuff/tree/main/qwen2_deploy">https://github.com/CurtisNewbie/llm_stuff/tree/main/qwen2_deploy</a></em></strong></p>

<p>You may follow other official guides, e.g., to install Pytorch properly using venv, conda or something else.</p>

<ul>
  <li>https://pytorch.org/get-started/previous-versions/</li>
</ul>

<p>Install nvidia driver, after the installation, check your GPU status using <code class="language-plaintext highlighter-rouge">nvidia-smi</code> command:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvidia-smi

<span class="c"># *** ***  * **:**:** 2024</span>
<span class="c"># +---------------------------------------------------------------------------------------+</span>
<span class="c"># | NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |</span>
<span class="c"># |-----------------------------------------+----------------------+----------------------+</span>
<span class="c"># | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |</span>
<span class="c"># | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |</span>
<span class="c"># |                                         |                      |               MIG M. |</span>
<span class="c"># |=========================================+======================+======================|</span>
<span class="c"># |   0  NVIDIA A10                     On  | 00000000:00:07.0 Off |                    0 |</span>
<span class="c"># |  0%   26C    P8               8W / 150W |      0MiB / 23028MiB |      0%      Default |</span>
<span class="c"># |                                         |                      |                  N/A |</span>
<span class="c"># +-----------------------------------------+----------------------+----------------------+</span>

<span class="c"># +---------------------------------------------------------------------------------------+</span>
<span class="c"># | Processes:                                                                            |</span>
<span class="c"># |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |</span>
<span class="c"># |        ID   ID                                                             Usage      |</span>
<span class="c"># |=======================================================================================|</span>
<span class="c"># |  No running processes found                                                           |</span>
<span class="c"># +---------------------------------------------------------------------------------------+</span>
</code></pre></div></div>

<p>The output above shows that we have a Nvidia A10 GPU installed with 23GB of VRAM available. Notice that even though the output of <code class="language-plaintext highlighter-rouge">nvidia-smi</code> shows <code class="language-plaintext highlighter-rouge">CUDA Version: 12.2</code>, it’s merely a compatible driver version.</p>

<p>If you don’t have CUDA already, install CUDA toolkit instead of CUDA. CUDA is included in CUDA toolkit. Notice that we are using <code class="language-plaintext highlighter-rouge">CUDA12.1</code>, we should make sure that all relavent cuda dependencies support exactly CUDA 12.1.</p>

<ul>
  <li><a href="https://developer.nvidia.com/cuda-12-0-0-download-archive?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=CentOS&amp;target_version=7&amp;target_type=rpm_network">Nvidia CUDA Toolkit 12.0 Downloads</a></li>
  <li><a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html">Nvidia CUDA Toolkit Release Notes</a></li>
  <li><a href="https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/">Nvidia rhel7/x86_64 CUDA Repo Index</a></li>
</ul>

<p>Since we are using CentOS, we use yum package manager. In the above Nvidia websites: select your OS, architecture and distribution; copy paste the generated installation instructions:
(it seems like you don’t actually need <code class="language-plaintext highlighter-rouge">nvidia-driver-latest-dkms</code>, the commands below are exactly what I did anyway).</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Add cuda repo config</span>
<span class="nb">sudo </span>yum-config-manager <span class="nt">--add-repo</span> https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/cuda-rhel7.repo

<span class="c"># clean cache</span>
<span class="nb">sudo </span>yum clean all

<span class="c"># install cuda toolkit, cuda is included</span>
<span class="nb">sudo </span>yum <span class="nt">-y</span> <span class="nb">install </span>cuda-toolkit
</code></pre></div></div>

<p>Check that we have <code class="language-plaintext highlighter-rouge">nvcc</code> installed, <code class="language-plaintext highlighter-rouge">nvcc</code> is Nvidia CUDA Compiler Driver, it should be automatically installed as part of the CUDA toolkit.</p>

<ul>
  <li>https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/</li>
</ul>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvcc <span class="nt">--version</span>
<span class="c"># nvcc: NVIDIA (R) Cuda compiler driver</span>
<span class="c"># Copyright (c) 2005-2023 NVIDIA Corporation</span>
<span class="c"># Built on Mon_Apr__3_17:16:06_PDT_2023</span>
<span class="c"># Cuda compilation tools, release 12.1, V12.1.105</span>
<span class="c"># Build cuda_12.1.r12.1/compiler.32688072_0</span>
</code></pre></div></div>

<p>Install <code class="language-plaintext highlighter-rouge">nccl</code> library. <code class="language-plaintext highlighter-rouge">nccl</code> is NVIDIA Collective Communications Library. PyTorch will attempt to open these <code class="language-plaintext highlighter-rouge">*.so.*</code> lib files provided by nccl.</p>

<ul>
  <li><a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/overview.html#:~:text=The%20NVIDIA%20Collective%20Communications%20Library,be%20easily%20integrated%20into%20applications.">Nvidia Overview of NCCL</a></li>
  <li><a href="https://docs.nvidia.com/deeplearning/nccl/install-guide/index.html">Nvidia NCCL Installation Guide</a></li>
  <li><a href="https://developer.nvidia.com/nccl">Nvidia NCCL</a></li>
</ul>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Add cuda repo config (same repo)</span>
<span class="nb">sudo </span>yum-config-manager <span class="nt">--add-repo</span> https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/cuda-rhel7.repo

<span class="c"># Install libnccl for cuda12.1</span>
yum <span class="nb">install </span>libnccl-2.18.3-1+cuda12.1

yum list installed | <span class="nb">grep </span>nccl
<span class="c"># libnccl.x86_64                                     2.18.3-1+cuda12.1                   @cuda-rhel7-x86_64</span>
</code></pre></div></div>

<p>Have a look at cuda lib location, it should be located at <code class="language-plaintext highlighter-rouge">/usr/local</code></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">ls</span> <span class="nt">-l</span> /usr/local/ | <span class="nb">grep </span>cuda
<span class="c"># lrwxrwxrwx   1 root root   21 Jun 28 15:15 cuda -&gt; /usr/local/cuda-12.1/</span>
<span class="c"># drwxr-xr-x  17 root root 4096 Jun 28 15:16 cuda-12.1</span>
</code></pre></div></div>

<p>Install python3.11 (or later versions):</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>yum <span class="nb">install </span>python3.11
python3.11 <span class="nt">-m</span> ensurepip
</code></pre></div></div>

<p>As shown below, I wrote two python scripts: one is for loading documents to Qdrant (vector database), another one is for bootstraping the RAG LLM model to server QA queries.
(The model used in this example: <a href="https://huggingface.co/Qwen/Qwen2-7B-Instruct">huggingface.co/Qwen/Qwen2-7B-Instruct</a>)</p>

<p><code class="language-plaintext highlighter-rouge">llm.py</code></p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ...
</span><span class="n">collection_name</span> <span class="o">=</span> <span class="s">"documents"</span>
<span class="n">embedding_name</span> <span class="o">=</span> <span class="s">"sentence-transformers/distiluse-base-multilingual-cased-v1"</span>
<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">task</span> <span class="o">=</span> <span class="s">"text-generation"</span>
<span class="n">model</span> <span class="o">=</span> <span class="s">"Qwen/Qwen2-7B-Instruct"</span>
<span class="n">cache_path</span> <span class="o">=</span> <span class="s">"/root/qdrant_cache"</span>

<span class="n">hf</span> <span class="o">=</span> <span class="n">HuggingFacePipeline</span><span class="p">.</span><span class="n">from_model_id</span><span class="p">(</span>
    <span class="n">model_id</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">,</span>
    <span class="n">pipeline_kwargs</span><span class="o">=</span><span class="p">{</span>
        <span class="s">"max_new_tokens"</span><span class="p">:</span> <span class="n">max_new_tokens</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="n">model_kwargs</span><span class="o">=</span><span class="p">{</span>
        <span class="s">"temperature"</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">,</span>
        <span class="s">"top_k"</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
        <span class="s">"top_p"</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">,</span>
        <span class="s">"do_sample"</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
        <span class="s">"load_in_4bit"</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">HuggingFaceEmbeddings</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="n">embedding_name</span><span class="p">)</span>
<span class="n">vectorstore</span> <span class="o">=</span> <span class="n">Qdrant</span><span class="p">.</span><span class="n">from_existing_collection</span><span class="p">(</span>
    <span class="n">embedding</span><span class="o">=</span><span class="n">embeddings</span><span class="p">,</span>
    <span class="n">path</span><span class="o">=</span><span class="n">cache_path</span><span class="p">,</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="n">collection_name</span><span class="p">)</span>

<span class="n">template</span> <span class="o">=</span> <span class="s">"""...."""</span>
<span class="n">retriever</span> <span class="o">=</span> <span class="n">vectorstore</span><span class="p">.</span><span class="n">as_retriever</span><span class="p">(</span><span class="n">search_type</span><span class="o">=</span><span class="s">"similarity_score_threshold"</span><span class="p">,</span> <span class="n">search_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s">"score_threshold"</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">})</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>
<span class="n">chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span><span class="s">"context"</span><span class="p">:</span> <span class="n">retriever</span> <span class="o">|</span> <span class="n">format_docs</span><span class="p">,</span> <span class="s">"question"</span><span class="p">:</span> <span class="n">RunnablePassthrough</span><span class="p">()}</span>
    <span class="o">|</span> <span class="n">prompt</span>
    <span class="o">|</span> <span class="n">hf</span><span class="p">.</span><span class="n">bind</span><span class="p">()</span>
<span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">chain</span><span class="p">.</span><span class="n">invoke</span><span class="p">(</span><span class="s">"How is the weather today?"</span><span class="p">))</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">qdrant_load.py</code></p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ...
</span><span class="n">collection_name</span> <span class="o">=</span> <span class="s">"documents"</span>
<span class="n">embedding_name</span> <span class="o">=</span> <span class="s">"sentence-transformers/distiluse-base-multilingual-cased-v1"</span>
<span class="n">cache_path</span> <span class="o">=</span> <span class="s">"/root/qdrant_cache"</span>
<span class="n">base_dir</span> <span class="o">=</span> <span class="s">'/root/llm/files'</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="nb">file</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">base_dir</span><span class="p">):</span>
    <span class="n">file_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_dir</span><span class="p">,</span> <span class="nb">file</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">file</span><span class="p">.</span><span class="n">endswith</span><span class="p">(</span><span class="s">'.pdf'</span><span class="p">):</span>
        <span class="n">documents</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">PyPDFLoader</span><span class="p">(</span><span class="n">file_path</span><span class="p">).</span><span class="n">load</span><span class="p">())</span>
    <span class="k">elif</span> <span class="nb">file</span><span class="p">.</span><span class="n">endswith</span><span class="p">(</span><span class="s">'.docx'</span><span class="p">):</span>
        <span class="n">documents</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">Docx2txtLoader</span><span class="p">(</span><span class="n">file_path</span><span class="p">).</span><span class="n">load</span><span class="p">())</span>
    <span class="k">elif</span> <span class="nb">file</span><span class="p">.</span><span class="n">endswith</span><span class="p">(</span><span class="s">'.txt'</span><span class="p">):</span>
        <span class="n">documents</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">TextLoader</span><span class="p">(</span><span class="n">file_path</span><span class="p">).</span><span class="n">load</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Documents loaded"</span><span class="p">)</span>

<span class="n">text_splitter</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">chunked_documents</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="p">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Documents splited"</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">HuggingFaceEmbeddings</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="n">embedding_name</span><span class="p">,</span> <span class="n">show_progress</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Embeddings loaded"</span><span class="p">)</span>

<span class="n">vectorstore</span> <span class="o">=</span> <span class="n">Qdrant</span><span class="p">.</span><span class="n">from_existing_collection</span><span class="p">(</span>
    <span class="n">embedding</span><span class="o">=</span><span class="n">embeddings</span><span class="p">,</span>
    <span class="n">path</span><span class="o">=</span><span class="n">cache_path</span><span class="p">,</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="n">collection_name</span><span class="p">)</span>

<span class="n">vectorstore</span><span class="p">.</span><span class="n">add_documents</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Documents added"</span><span class="p">)</span>
</code></pre></div></div>

<p>Install python dependencies:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3.11 <span class="nt">-m</span> pip <span class="nb">install </span>langchain langchain-huggingface transformers bitsandbytes langchain-community langchain-qdrant accelerate
</code></pre></div></div>

<p>If everything goes right, we can use <code class="language-plaintext highlighter-rouge">qdrant_load.py</code> to load documents, and then we can use <code class="language-plaintext highlighter-rouge">llm.py</code> to load the RAG LLM Model to answer questions.</p>

<p>To create simple ChatUI, we can use <code class="language-plaintext highlighter-rouge">gradio</code>.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3.11 <span class="nt">-m</span> pip <span class="nb">install </span>gradio
</code></pre></div></div>

<p>Change <code class="language-plaintext highlighter-rouge">llm.py</code> to use gradio to serve QA at <code class="language-plaintext highlighter-rouge">0.0.0.0:80</code>:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">gradio</span> <span class="k">as</span> <span class="n">gr</span>

<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">history</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">message</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="s">"Say something?"</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">chain</span><span class="p">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
    <span class="k">yield</span> <span class="n">result</span>

<span class="n">gr</span><span class="p">.</span><span class="n">ChatInterface</span><span class="p">(</span><span class="n">predict</span><span class="p">).</span><span class="n">queue</span><span class="p">().</span><span class="n">launch</span><span class="p">(</span><span class="n">share</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">server_name</span><span class="o">=</span><span class="s">"0.0.0.0"</span><span class="p">,</span> <span class="n">server_port</span><span class="o">=</span><span class="mi">80</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="potential-issues">Potential Issues</h3>

<h4 id="updates-in-2025-about-libcuptiso-files-not-found">Updates in 2025 about libcupti.so.* files not found</h4>

<p><strong>LD_LIBRARY_PATH actually works, with env LD_LIBRARY_PATH properly set, you don’t need to install nccl.</strong></p>

<p>E.g.,</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span><span class="k">***</span> src]# find /usr/ <span class="nt">-name</span> <span class="s2">"libnccl.so.2"</span>
/usr/local/lib/python3.11/site-packages/nvidia/nccl/lib/libnccl.so.2

<span class="o">[</span><span class="k">***</span> src]# find /usr/ <span class="nt">-name</span> <span class="s2">"libcupti.so.12"</span>
/usr/local/cuda-12.4/extras/CUPTI/lib64/libcupti.so.12
/usr/local/lib/python3.11/site-packages/nvidia/cuda_cupti/lib/libcupti.so.12

<span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="s2">"/usr/local/lib/python3.11/site-packages/nvidia/cuda_cupti/lib:/usr/local/lib/python3.11/site-packages/nvidia/nccl/lib:/usr/local/cuda-12.4:</span><span class="nv">$LD_LIBRARY_PATH</span><span class="s2">"</span>

<span class="c"># check if pytorch works</span>
<span class="o">[</span><span class="k">***</span> src]# python3.11 <span class="nt">-c</span> <span class="s2">"import torch; print(torch.cuda.nccl.version())"</span>
<span class="o">(</span>2, 21, 5<span class="o">)</span>
<span class="o">[</span><span class="k">***</span> src]# python3.11 <span class="nt">-c</span> <span class="s2">"import torch; print(torch.cuda.is_available())"</span>
True
</code></pre></div></div>

<h4 id="1-pytorch---importerror-libcuptiso12-cannot-open-shared-object-file-no-such-file-or-directory">1. PyTorch - <code class="language-plaintext highlighter-rouge">ImportError: libcupti.so.12: cannot open shared object file: No such file or directory</code></h4>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>...

Traceback <span class="o">(</span>most recent call last<span class="o">)</span>:
  File <span class="s2">"/usr/local/lib/python3.11/site-packages/langchain_huggingface/embeddings/huggingface.py"</span>, line 53, <span class="k">in </span>__init__
    import sentence_transformers  <span class="c"># type: ignore[import]</span>
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File <span class="s2">"/usr/local/lib/python3.11/site-packages/sentence_transformers/__init__.py"</span>, line 7, <span class="k">in</span> &lt;module&gt;
    from sentence_transformers.cross_encoder.CrossEncoder import CrossEncoder
  File <span class="s2">"/usr/local/lib/python3.11/site-packages/sentence_transformers/cross_encoder/__init__.py"</span>, line 1, <span class="k">in</span> &lt;module&gt;
    from .CrossEncoder import CrossEncoder
  File <span class="s2">"/usr/local/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py"</span>, line 7, <span class="k">in</span> &lt;module&gt;
    import torch
  File <span class="s2">"/usr/local/lib64/python3.11/site-packages/torch/__init__.py"</span>, line 239, <span class="k">in</span> &lt;module&gt;
    from torch._C import <span class="k">*</span>  <span class="c"># noqa: F403</span>
    ^^^^^^^^^^^^^^^^^^^^^^
ImportError: libcupti.so.12: cannot open shared object file: No such file or directory
</code></pre></div></div>

<p>This issue is actually caused by PyTorch. Notice that almost everything is built on top of PyTorch, and PyTorch internally communicates with Nvidia GPU using CUDA.
<code class="language-plaintext highlighter-rouge">libcupti.so.12</code> is a lib file provided by CUDA.</p>

<p>The file may exist in your OS, but PyTorch somehow couldn’t find it. Use <code class="language-plaintext highlighter-rouge">find</code> to locate the lib file first:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>find / <span class="nt">-name</span> <span class="s2">"libcupti.so.12"</span>
<span class="c"># /usr/local/cuda-12.1/extras/CUPTI/lib64/libcupti.so.12</span>
</code></pre></div></div>

<p>Create symbolic link to python3.11 package path so that PyTorch can correctly open the lib files:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir</span> <span class="nt">-p</span> /usr/local/lib64/python3.11/site-packages/nvidia/nccl/lib
<span class="nb">ln</span> <span class="nt">-s</span> /usr/local/cuda-12.1/extras/CUPTI/lib64 /usr/local/lib64/python3.11/site-packages/nvidia/nccl/lib

<span class="c"># just in case you want to remove the symlink:</span>
<span class="c"># unlink  /usr/local/lib64/python3.11/site-packages/nvidia/nccl/lib</span>
</code></pre></div></div>

<p>Before I tried the symlink approch, I did some research; some recommend setting <code class="language-plaintext highlighter-rouge">LD_LIBRARY_PATH</code> env variable like below, but unfortunately, this did not work for me.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/usr/local/cuda-12.1/extras/CUPTI/lib64/:<span class="nv">$LD_LIBRARY_PATH</span>
</code></pre></div></div>

<h4 id="2-pytorch---importerror-libncclso2-cannot-open-shared-object-file-no-such-file-or-directory">2. PyTorch - <code class="language-plaintext highlighter-rouge">ImportError: libnccl.so.2: cannot open shared object file: No such file or directory</code></h4>

<p>This is still a PyTorch problem, the output tracktrace is almost the same. This is due to incompatible PyTorch version.</p>

<p>Some recommend to simply downgrade the PyTorch version, or build the PyTorch on your own.</p>

<ul>
  <li>https://github.com/pytorch/pytorch/issues/88802</li>
  <li>https://discuss.pytorch.org/t/pytorch-for-cuda-12/169447/20?page=2</li>
</ul>

<p>So what I did was simply downgrading my PyTorch from <code class="language-plaintext highlighter-rouge">2.3.1</code> to <code class="language-plaintext highlighter-rouge">2.1.1</code>:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3.11 <span class="nt">-m</span> pip <span class="nb">install </span>torch-<span class="o">=</span>2.1.1
</code></pre></div></div>

<h4 id="3-pytorch---importerror-site-packagestorchliblibtorch_cudaso-undefined-symbol-ncclcommregister">3. PyTorch - <code class="language-plaintext highlighter-rouge">ImportError: .../site-packages/torch/lib/libtorch_cuda.so: undefined symbol: ncclCommRegister</code></h4>

<p>This issue arrived when I tried to downgrade PyTorch from <code class="language-plaintext highlighter-rouge">2.3.1</code> to <code class="language-plaintext highlighter-rouge">2.3.0</code>. This is due to incompatible PyTorch version.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Traceback <span class="o">(</span>most recent call last<span class="o">)</span>:
  File <span class="s2">"/usr/local/lib/python3.11/site-packages/langchain_huggingface/embeddings/huggingface.py"</span>, line 53, <span class="k">in </span>__init__
    import sentence_transformers  <span class="c"># type: ignore[import]</span>
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File <span class="s2">"/usr/local/lib/python3.11/site-packages/sentence_transformers/__init__.py"</span>, line 7, <span class="k">in</span> &lt;module&gt;
    from sentence_transformers.cross_encoder.CrossEncoder import CrossEncoder
  File <span class="s2">"/usr/local/lib/python3.11/site-packages/sentence_transformers/cross_encoder/__init__.py"</span>, line 1, <span class="k">in</span> &lt;module&gt;
    from .CrossEncoder import CrossEncoder
  File <span class="s2">"/usr/local/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py"</span>, line 7, <span class="k">in</span> &lt;module&gt;
    import torch
  File <span class="s2">"/usr/local/lib64/python3.11/site-packages/torch/__init__.py"</span>, line 237, <span class="k">in</span> &lt;module&gt;
    from torch._C import <span class="k">*</span>  <span class="c"># noqa: F403</span>
    ^^^^^^^^^^^^^^^^^^^^^^
ImportError: /usr/local/lib64/python3.11/site-packages/torch/lib/libtorch_cuda.so: undefined symbol: ncclCommRegister
</code></pre></div></div>

<p>The way you fix the problem is the same as the issue 2: downgrading your PyTorch to an older version like <code class="language-plaintext highlighter-rouge">2.1.1</code>.</p>

<h4 id="4-huggingface-connectivity-issue">4. HuggingFace Connectivity Issue.</h4>

<p>If you are from countries where HuggingFace is not really accesible. You can set <code class="language-plaintext highlighter-rouge">HF_ENDPOINT</code> environment variable to a mirror repo that is accessible from your country.</p>

<p>E.g.,</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">HF_ENDPOINT</span><span class="o">=</span>https://hf-mirror.com
</code></pre></div></div>

<h2 id="conceptual-guide">Conceptual Guide</h2>

<ul>
  <li><a href="https://python.langchain.com/v0.2/docs/concepts">LangChain - Conceptual Guide</a></li>
</ul>

<p>TODO:</p>

<h2 id="conversational-rag">Conversational RAG</h2>

<p>Conversational RAG maintains history of conversation.</p>

<ul>
  <li><a href="https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/">LangChain - Conversational RAG</a></li>
</ul>

<p>TODO: How to remmeber the chat history</p>

<h2 id="terminology">Terminology</h2>

<ul>
  <li><code class="language-plaintext highlighter-rouge">HF Format</code>: Hugging Face Format</li>
</ul>

  </div><a class="u-url" href="/learning/2024/06/24/explore-llm-using-langchain-and-huggingface.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">kekw</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Yongjie Zhuang</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/CurtisNewbie"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">CurtisNewbie</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Til the end</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>